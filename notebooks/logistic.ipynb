# %% [markdown]
# Titanic: Train/Test Split, Cross-Validation, and Logistic Regression
#
# Instructor: Imran Ali — Data Science Tools & Techniques (3-hour class — first half)
#
# This notebook explains how data is split into train and test sets, what cross-validation is
# and why it's important, then builds a logistic regression pipeline for the Titanic dataset.
# The final section contains a short set of student exercises to solidify learning.

# %% [markdown]
# ## Setup: imports and dataset

# %%
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,
                             roc_auc_score, RocCurveDisplay, precision_score, recall_score, f1_score)

sns.set(style='whitegrid')

# Load Titanic dataset from seaborn (convenient for classroom use)
# Note: if you prefer using your prepared version of Titanic, replace this cell with your file loading.

titanic = sns.load_dataset('titanic')
print("Columns available:", titanic.columns.tolist())

titanic.head()

# %% [markdown]
# ## Quick data preparation (select a small, teachable feature set)
# We'll use a concise set of features that are commonly used with the Titanic dataset:
# `pclass`, `sex`, `age`, `sibsp`, `parch`, `fare`, `embarked`.
# Target: `survived`.

# %%
features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']
target = 'survived'

df = titanic[features + [target]].copy()
print("Rows before dropping NA (for demonstration):", df.shape[0])

# We'll keep the dataset as-is and show how pipeline handles missing values.
df.sample(5)

# %% [markdown]
# ## 1) Train / Test Split
#
# **Why split?**
# - Training data is what the model learns from. The model will adjust parameters to minimize error on this set.
# - Test data is **held out** and used only to evaluate how well the model generalizes to unseen data.
# - If we evaluate on the training set, we may be measuring how well the model memorized the data (overfitting).
#
# **Stratified split:** When the target is imbalanced (unequal class frequencies), stratification preserves the
# class distribution in train and test sets. Titanic survival is a binary target — use `stratify=y`.

# %%
X = df[features]
y = df[target]

# Stratified split: keeps proportion of survived/not survived the same in train and test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train size:", X_train.shape)
print("Test size:", X_test.shape)
print("Train target distribution:\n", y_train.value_counts(normalize=True))
print("Test target distribution:\n", y_test.value_counts(normalize=True))

# %% [markdown]
# ## 2) Cross-Validation (CV)
#
# **What is cross-validation?**
# - Cross-validation splits the training data into several (k) folds. The model is trained `k` times, each time
#   using k-1 folds for training and 1 fold for validation.
# - The final CV score is typically the average of the validation scores.
#
# **Why CV is important:**
# - Provides a more robust estimate of how a model performs on unseen data (compared to a single train/test split).
# - Helps detect high variance models (models that perform well on some folds and poorly on others — overfitting).
# - Useful when data is limited — CV uses more of the data for training on each run.
#
# **Stratified K-Fold:** For classification tasks, use `StratifiedKFold` to preserve class proportions in each fold.

# %%
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
print(skf)

# %% [markdown]
# ## 3) Preprocessing + Logistic Regression Pipeline
# We'll build a `ColumnTransformer` to:
# - Impute missing numeric values with the median
# - Scale numeric features
# - Impute missing categorical values and one-hot encode them
#
# Then create a pipeline: `preprocessor -> LogisticRegression`.

# %%
numeric_features = ['age', 'sibsp', 'parch', 'fare', 'pclass']  # pclass is numeric but discrete
categorical_features = ['sex', 'embarked']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

# Full pipeline
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('clf', LogisticRegression(max_iter=1000))
])

# %% [markdown]
# ### Fit the pipeline on the training set and evaluate on the test set

# %%
pipe.fit(X_train, y_train)

# Predict on the held-out test set
y_pred = pipe.predict(X_test)

print("Test Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

# %% [markdown]
# ### Predicted probabilities and ROC AUC
# Logistic regression returns probabilities via `predict_proba` — useful when you need "confidence" or want to
# set a decision threshold different from 0.5.

# %%
probas = pipe.predict_proba(X_test)[:, 1]
print('Sample probabilities:', probas[:5])
print('ROC AUC on test set:', roc_auc_score(y_test, probas))

# Plot ROC curve
RocCurveDisplay.from_estimator(pipe, X_test, y_test)
plt.title('ROC curve — Logistic Regression')
plt.show()

# %% [markdown]
# ## 4) Cross-Validation score (on training data)
# Use stratified K-fold CV to estimate expected performance. We use `cross_val_score` with the pipeline.

# %%
cv_scores = cross_val_score(pipe, X_train, y_train, cv=skf, scoring='accuracy')
print('Cross-validation accuracies:', np.round(cv_scores, 4))
print('Mean CV accuracy: {:.4f} (+/- {:.4f})'.format(cv_scores.mean(), cv_scores.std()))

# %% [markdown]
# ## 5) Hyperparameter tuning with GridSearchCV
# We'll tune `C` (inverse regularization strength) for LogisticRegression. We still use Stratified K-Fold.

# %%
param_grid = {
    'clf__C': [0.01, 0.1, 1, 10, 100],
    'clf__penalty': ['l2'],  # default; use 'liblinear' solver for small datasets if using 'l1'
}

grid = GridSearchCV(pipe, param_grid, cv=skf, scoring='accuracy', n_jobs=-1)

grid.fit(X_train, y_train)

print('Best params:', grid.best_params_)
print('Best CV score:', grid.best_score_)

# Evaluate best estimator on test set
best = grid.best_estimator_
y_pred_best = best.predict(X_test)
print('\nTest accuracy (best estimator):', accuracy_score(y_test, y_pred_best))
print('\nClassification report (best estimator):\n', classification_report(y_test, y_pred_best))

# %% [markdown]
# ## 6) Quick discussion points (to explain in class)
# - Why stratify during `train_test_split` and in `StratifiedKFold`?
#   - Ensures training, validation, and test sets have similar class proportions -> stable, comparable scores.
# - Why use pipelines?
#   - Encapsulates preprocessing + model so that transformations are fit only on training data and applied to test data
#     correctly (prevents data leakage).
# - Why cross-validation before final test evaluation?
#   - CV helps choose hyperparameters and understand variance; final test is used only once for a realistic estimate.
# - What does regularization (`C` in logistic regression) do?
#   - Penalizes large weights to reduce overfitting. Smaller `C` -> stronger regularization.

# %% [markdown]
# ## 7) Student Exercises (In-class practice + take-home)
#
# **In-class (30–40 minutes, guided):**
# 1. Using the pipeline above, compute **precision**, **recall**, and **F1-score** on the test set and explain
#    which metric is more important for Titanic and why.
# 2. Plot the confusion matrix as a heatmap and interpret false positives vs false negatives in the context of the
#    Titanic dataset.
# 3. Run `cross_val_score` with `scoring='roc_auc'` and compare mean AUC to mean accuracy. Does it change the
#    ranking of models if you try another model?
#
# **Stretch / Take-home (for deeper practice):**
# 1. Train a `DecisionTreeClassifier` (max_depth=3) inside a pipeline and compare its CV accuracy and test
#    accuracy with logistic regression. Visualize the tree.
# 2. Train a `RandomForestClassifier` and use `GridSearchCV` to tune `n_estimators` and `max_depth`.
#    Compare feature importances with the logistic regression coefficients (after one-hot encoding).
# 3. (Bonus) Use `StratifiedKFold` to create out-of-fold predictions for the training data and plot the calibration
#    curve of the logistic regression probabilities (are they well-calibrated?).

# %% [markdown]
# ## 8) Short answer / mini-assignment to submit (one paragraph + code)
# Ask each student to submit a short Jupyter cell (or a plain text file) containing:
# 1. A single plot: confusion matrix heatmap for the **best** model they selected.
# 2. Two lines: the model's test accuracy and ROC AUC.
# 3. A 3–4 sentence interpretation: Which model would you choose for Titanic and why? (mention one metric and
#    one qualitative reason — interpretability, training time, robustness to outliers, etc.)

# %% [markdown]
# ## End of notebook
# Feel free to adapt features, replace the data-loading step with your prepared Titanic CSV, or extend the
# exercises for homework. If you want, I can also convert this into a ready `.ipynb` file for distribution.

